{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    },
    "colab": {
      "name": "Machine Learning with MLlib.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/waltz2u/bd/blob/master/Machine_Learning_with_MLlib.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLwZb8cNBD0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKFCEJ7MBjst",
        "colab_type": "text"
      },
      "source": [
        "# Setting things up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2SKAh0RsB20J",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "outputId": "81e76fcd-d331-45bf-dd17-85a957075562"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-2.4.5/spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.5-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [Connecting to archive.ubuntu.com] [Connecting to security.ubuntu.com (91.18\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com] [Connecting to\r                                                                               \rIgn:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [1 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.152)\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [Wa\r                                                                               \rIgn:3 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.152)] [Waiting for headers] [Wa\r                                                                               \rHit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Waiting for headers] [Waitin\r                                                                               \rHit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [Waiting for headers] [Waitin\r                                                                               \rGet:6 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r0% [Release.gpg gpgv 564 B] [Waiting for headers] [6 InRelease 2,586 B/88.7 kB \r                                                                               \rHit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:12 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [832 kB]\n",
            "Get:14 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,784 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,151 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [857 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,361 kB]\n",
            "Get:19 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [861 kB]\n",
            "Fetched 7,117 kB in 6s (1,205 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkTO0aCCBzFC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f535057-3eff-41e4-e7e9-2f40f37a4f8e"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gdrive\tsample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UzyFmhqCPdb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "outputId": "785c37ad-113b-4219-bc39-efcdd72390e4"
      },
      "source": [
        "!update-alternatives --config java"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 2 choices for the alternative java (providing /usr/bin/java).\n",
            "\n",
            "  Selection    Path                                            Priority   Status\n",
            "------------------------------------------------------------\n",
            "* 0            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      auto mode\n",
            "  1            /usr/lib/jvm/java-11-openjdk-amd64/bin/java      1111      manual mode\n",
            "  2            /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java   1081      manual mode\n",
            "\n",
            "Press <enter> to keep the current choice[*], or type selection number: 2\n",
            "update-alternatives: using /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java to provide /usr/bin/java (java) in manual mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KONmD3T3Blvi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "30cbcb2e-32e2-48b4-aadc-61770bba1932"
      },
      "source": [
        "!java -version "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"11.0.6\" 2020-01-14\n",
            "OpenJDK Runtime Environment (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1)\n",
            "OpenJDK 64-Bit Server VM (build 11.0.6+10-post-Ubuntu-1ubuntu118.04.1, mixed mode, sharing)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZTTdnsGCYRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.5-bin-hadoop2.7\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeZkFbaoAhja",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning with MLlib "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msyzsyS-Ahji",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import findspark\n",
        "findspark.init(\"/content/spark-2.4.5-bin-hadoop2.7\")\n",
        "from pyspark.sql import SparkSession\n",
        "# Sets the Spark master URL to connect to, such as \"local\" to run locally, \n",
        "# \"local[4]\" to run locally with 4 cores, or \"spark://master:7077\" to run on a Spark standalone cluster.\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate() #"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ9wxjDkAhjr",
        "colab_type": "text"
      },
      "source": [
        "## Decision tree classifier\n",
        "\n",
        "Decision trees are a popular family of classification and regression methods. More information about the spark.ml implementation can be found further in the section on decision trees.\n",
        "\n",
        "The following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set. We use two feature transformers to prepare the data; these help index categories for the label and categorical features, adding metadata to the DataFrame which the Decision Tree algorithm can recognize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnKeajHyAhju",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "d70bd815-9017-4385-ab65-73c214bce2c8"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import DecisionTreeClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load the data stored in LIBSVM format as a DataFrame.\n",
        "data = spark.read.format(\"libsvm\").load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexers and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g \" % (1.0 - accuracy))\n",
        "\n",
        "treeModel = model.stages[2]\n",
        "# summary only\n",
        "print(treeModel)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------+--------------------+\n",
            "|prediction|indexedLabel|            features|\n",
            "+----------+------------+--------------------+\n",
            "|       1.0|         1.0|(692,[121,122,123...|\n",
            "|       1.0|         1.0|(692,[122,123,124...|\n",
            "|       1.0|         1.0|(692,[123,124,125...|\n",
            "|       1.0|         1.0|(692,[123,124,125...|\n",
            "|       1.0|         1.0|(692,[124,125,126...|\n",
            "+----------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Error = 0 \n",
            "DecisionTreeClassificationModel (uid=DecisionTreeClassifier_031d4e49167b) of depth 2 with 5 nodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGZDkficAhj4",
        "colab_type": "text"
      },
      "source": [
        "## Random forest classifier\n",
        "\n",
        "Random forests are a popular family of classification and regression methods. More information about the spark.ml implementation can be found further in the section on random forests.\n",
        "\n",
        "The following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set. We use two feature transformers to prepare the data; these help index categories for the label and categorical features, adding metadata to the DataFrame which the tree-based algorithms can recognize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2uJ-HByAhj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "7110ee27-eda2-42d6-ab55-7f0fbfbb5d3a"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load and parse the data file, converting it to a DataFrame.\n",
        "data = spark.read.format(\"libsvm\").load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "\n",
        "# Automatically identify categorical features, and index them.\n",
        "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a RandomForest model.\n",
        "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=10)\n",
        "\n",
        "# Convert indexed labels back to original labels.\n",
        "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
        "                               labels=labelIndexer.labels)\n",
        "\n",
        "# Chain indexers and forest in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"predictedLabel\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
        "\n",
        "rfModel = model.stages[2]\n",
        "print(rfModel)  # summary only"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+--------------+-----+--------------------+\n",
            "|predictedLabel|label|            features|\n",
            "+--------------+-----+--------------------+\n",
            "|           0.0|  0.0|(692,[124,125,126...|\n",
            "|           0.0|  0.0|(692,[126,127,128...|\n",
            "|           0.0|  0.0|(692,[126,127,128...|\n",
            "|           0.0|  0.0|(692,[126,127,128...|\n",
            "|           0.0|  0.0|(692,[127,128,129...|\n",
            "+--------------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Error = 0\n",
            "RandomForestClassificationModel (uid=RandomForestClassifier_e55b78ddeb1c) with 10 trees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "febxtD_AAhj_",
        "colab_type": "text"
      },
      "source": [
        "## Gradient-boosted tree classifier\n",
        "\n",
        "Gradient-boosted trees (GBTs) are a popular classification and regression method using ensembles of decision trees. More information about the spark.ml implementation can be found further in the section on GBTs.\n",
        "\n",
        "The following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set. We use two feature transformers to prepare the data; these help index categories for the label and categorical features, adding metadata to the DataFrame which the tree-based algorithms can recognize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnzAMApWAhkH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "dd00d908-c338-432b-edf3-3fb356f0fef3"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.classification import GBTClassifier\n",
        "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load and parse the data file, converting it to a DataFrame.\n",
        "data = spark.read.format(\"libsvm\").load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "# Index labels, adding metadata to the label column.\n",
        "# Fit on whole dataset to include all labels in index.\n",
        "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(data)\n",
        "# Automatically identify categorical features, and index them.\n",
        "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a GBT model.\n",
        "gbt = GBTClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", maxIter=10)\n",
        "\n",
        "# Chain indexers and GBT in a Pipeline\n",
        "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, gbt])\n",
        "\n",
        "# Train model.  This also runs the indexers.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = MulticlassClassificationEvaluator(\n",
        "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
        "\n",
        "gbtModel = model.stages[2]\n",
        "print(gbtModel)  # summary only"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+------------+--------------------+\n",
            "|prediction|indexedLabel|            features|\n",
            "+----------+------------+--------------------+\n",
            "|       1.0|         1.0|(692,[95,96,97,12...|\n",
            "|       1.0|         1.0|(692,[100,101,102...|\n",
            "|       1.0|         1.0|(692,[123,124,125...|\n",
            "|       1.0|         1.0|(692,[125,126,127...|\n",
            "|       1.0|         1.0|(692,[126,127,128...|\n",
            "+----------+------------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Test Error = 0.0322581\n",
            "GBTClassificationModel (uid=GBTClassifier_d08421dd79e8) with 10 trees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5O0IHlgAhkP",
        "colab_type": "text"
      },
      "source": [
        "## Multilayer perceptron classifier\n",
        "\n",
        "Multilayer perceptron classifier (MLPC) is a classifier based on the feedforward artificial neural network. MLPC consists of multiple layers of nodes. Each layer is fully connected to the next layer in the network. Nodes in the input layer represent the input data. All other nodes map inputs to outputs by a linear combination of the inputs with the node’s weights w and bias b and applying an activation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf-sKfBwAhkT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "16a5b903-52be-42a9-f601-dd1d8815b794"
      },
      "source": [
        "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load training data\n",
        "data = spark.read.format(\"libsvm\")\\\n",
        "    .load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\")\n",
        "\n",
        "# Split the data into train and test\n",
        "splits = data.randomSplit([0.6, 0.4], 1234)\n",
        "train = splits[0]\n",
        "test = splits[1]\n",
        "\n",
        "# specify layers for the neural network:\n",
        "# input layer of size 4 (features), two intermediate of size 5 and 4\n",
        "# and output of size 3 (classes)\n",
        "layers = [4, 5, 4, 3]\n",
        "\n",
        "# create the trainer and set its parameters\n",
        "trainer = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "\n",
        "# train the model\n",
        "model = trainer.fit(train)\n",
        "\n",
        "# compute accuracy on the test set\n",
        "result = model.transform(test)\n",
        "predictionAndLabels = result.select(\"prediction\", \"label\")\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set accuracy = 0.9019607843137255\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_Xso99iAhkZ",
        "colab_type": "text"
      },
      "source": [
        "## One-vs-Rest classifier (a.k.a. One-vs-All)\n",
        "\n",
        "OneVsRest is an example of a machine learning reduction for performing multiclass classification given a base classifier that can perform binary classification efficiently. It is also known as “One-vs-All.”\n",
        "\n",
        "OneVsRest is implemented as an Estimator. For the base classifier it takes instances of Classifier and creates a binary classification problem for each of the k classes. The classifier for class i is trained to predict whether the label is i or not, distinguishing class i from all other classes.\n",
        "\n",
        "Predictions are done by evaluating each binary classifier and the index of the most confident classifier is output as label.\n",
        "\n",
        "The example below demonstrates how to load the Iris dataset, parse it as a DataFrame and perform multiclass classification using OneVsRest. The test error is calculated to measure the algorithm accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ifxLPGjVAhkb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b0af349f-a433-416e-a3fd-cae2343bd1fd"
      },
      "source": [
        "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# load data file.\n",
        "inputData = spark.read.format(\"libsvm\") \\\n",
        "    .load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_multiclass_classification_data.txt\")\n",
        "\n",
        "# generate the train/test split.\n",
        "(train, test) = inputData.randomSplit([0.8, 0.2])\n",
        "\n",
        "# instantiate the base classifier.\n",
        "lr = LogisticRegression(maxIter=10, tol=1E-6, fitIntercept=True)\n",
        "\n",
        "# instantiate the One Vs Rest Classifier.\n",
        "ovr = OneVsRest(classifier=lr)\n",
        "\n",
        "# train the multiclass model.\n",
        "ovrModel = ovr.fit(train)\n",
        "\n",
        "# score the model on test data.\n",
        "predictions = ovrModel.transform(test)\n",
        "\n",
        "# obtain evaluator.\n",
        "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
        "\n",
        "# compute the classification error on test data.\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test Error = %g\" % (1.0 - accuracy))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Error = 0.027027\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93NtweLCAhko",
        "colab_type": "text"
      },
      "source": [
        "## Naive Bayes\n",
        "\n",
        "Naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes’ theorem with strong (naive) independence assumptions between the features. The spark.ml implementation currently supports both multinomial naive Bayes and Bernoulli naive Bayes. More information can be found in the section on Naive Bayes in MLlib."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSuDk2CYAhkt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 480
        },
        "outputId": "9a77c4a7-e5dd-4bf2-cee4-cde57637bec8"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# Load training data\n",
        "data = spark.read.format(\"libsvm\") \\\n",
        "    .load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "# Split the data into train and test\n",
        "splits = data.randomSplit([0.6, 0.4], 1234)\n",
        "train = splits[0]\n",
        "test = splits[1]\n",
        "\n",
        "# create the trainer and set its parameters\n",
        "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")\n",
        "\n",
        "# train the model\n",
        "model = nb.fit(train)\n",
        "\n",
        "# select example rows to display.\n",
        "predictions = model.transform(test)\n",
        "predictions.show()\n",
        "\n",
        "# compute accuracy on the test set\n",
        "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\",\n",
        "                                              metricName=\"accuracy\")\n",
        "accuracy = evaluator.evaluate(predictions)\n",
        "print(\"Test set accuracy = \" + str(accuracy))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+-----+--------------------+--------------------+-----------+----------+\n",
            "|label|            features|       rawPrediction|probability|prediction|\n",
            "+-----+--------------------+--------------------+-----------+----------+\n",
            "|  0.0|(692,[95,96,97,12...|[-174115.98587057...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[98,99,100,1...|[-178402.52307196...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[100,101,102...|[-100905.88974016...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[123,124,125...|[-244784.29791241...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[123,124,125...|[-196900.88506109...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[124,125,126...|[-238164.45338794...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[124,125,126...|[-184206.87833381...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[127,128,129...|[-214174.52863813...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[127,128,129...|[-182844.62193963...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[128,129,130...|[-246557.10990301...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[152,153,154...|[-208282.08496711...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[152,153,154...|[-243457.69885665...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[153,154,155...|[-260933.50931276...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[154,155,156...|[-220274.72552901...|  [1.0,0.0]|       0.0|\n",
            "|  0.0|(692,[181,182,183...|[-154830.07125175...|  [1.0,0.0]|       0.0|\n",
            "|  1.0|(692,[99,100,101,...|[-145978.24563975...|  [0.0,1.0]|       1.0|\n",
            "|  1.0|(692,[100,101,102...|[-147916.32657832...|  [0.0,1.0]|       1.0|\n",
            "|  1.0|(692,[123,124,125...|[-139663.27471685...|  [0.0,1.0]|       1.0|\n",
            "|  1.0|(692,[124,125,126...|[-129013.44238751...|  [0.0,1.0]|       1.0|\n",
            "|  1.0|(692,[125,126,127...|[-81829.799906049...|  [0.0,1.0]|       1.0|\n",
            "+-----+--------------------+--------------------+-----------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Test set accuracy = 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iim6mFwiAhk4",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression\n",
        "\n",
        "The interface for working with linear regression models and model summaries is similar to the logistic regression case.\n",
        "\n",
        "When fitting LinearRegressionModel without intercept on dataset with constant nonzero column by “l-bfgs” solver, Spark MLlib outputs zero coefficients for constant nonzero columns. This behavior is the same as R glmnet but different from LIBSVM.\n",
        "\n",
        "The following example demonstrates training an elastic net regularized linear regression model and extracting model summary statistics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpTwH0tlAhk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 586
        },
        "outputId": "482c4be9-2fd3-43b3-afdf-dc204f96611e"
      },
      "source": [
        "from pyspark.ml.regression import LinearRegression\n",
        "# Load training data\n",
        "training = spark.read.format(\"libsvm\")\\\n",
        "    .load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\")\n",
        "\n",
        "lr = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
        "\n",
        "# Fit the model\n",
        "lrModel = lr.fit(training)\n",
        "\n",
        "# Print the coefficients and intercept for linear regression\n",
        "print(\"Coefficients: %s\" % str(lrModel.coefficients))\n",
        "print(\"Intercept: %s\" % str(lrModel.intercept))\n",
        "\n",
        "# Summarize the model over the training set and print out some metrics\n",
        "trainingSummary = lrModel.summary\n",
        "print(\"numIterations: %d\" % trainingSummary.totalIterations)\n",
        "print(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\n",
        "trainingSummary.residuals.show()\n",
        "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
        "print(\"r2: %f\" % trainingSummary.r2)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients: [0.0,0.32292516677405936,-0.3438548034562218,1.9156017023458414,0.05288058680386263,0.765962720459771,0.0,-0.15105392669186682,-0.21587930360904642,0.22025369188813426]\n",
            "Intercept: 0.1598936844239736\n",
            "numIterations: 7\n",
            "objectiveHistory: [0.49999999999999994, 0.4967620357443381, 0.4936361664340463, 0.4936351537897608, 0.4936351214177871, 0.49363512062528014, 0.4936351206216114]\n",
            "+--------------------+\n",
            "|           residuals|\n",
            "+--------------------+\n",
            "|  -9.889232683103197|\n",
            "|  0.5533794340053554|\n",
            "|  -5.204019455758823|\n",
            "| -20.566686715507508|\n",
            "|    -9.4497405180564|\n",
            "|  -6.909112502719486|\n",
            "|  -10.00431602969873|\n",
            "|   2.062397807050484|\n",
            "|  3.1117508432954772|\n",
            "| -15.893608229419382|\n",
            "|  -5.036284254673026|\n",
            "|   6.483215876994333|\n",
            "|  12.429497299109002|\n",
            "|  -20.32003219007654|\n",
            "| -2.0049838218725005|\n",
            "| -17.867901734183793|\n",
            "|   7.646455887420495|\n",
            "| -2.2653482182417406|\n",
            "|-0.10308920436195645|\n",
            "|  -1.380034070385301|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "RMSE: 10.189077\n",
            "r2: 0.022861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqnswuHyAhlG",
        "colab_type": "text"
      },
      "source": [
        "## Generalized linear regression\n",
        "\n",
        "Contrasted with linear regression where the output is assumed to follow a Gaussian distribution, generalized linear models (GLMs) are specifications of linear models where the response variable Yi follows some distribution from the exponential family of distributions. Spark’s GeneralizedLinearRegression interface allows for flexible specification of GLMs which can be used for various types of prediction problems including linear regression, Poisson regression, logistic regression, and others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAklhKoMAhlJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "outputId": "c93ad3c5-84c8-4994-9a14-98e21c0e868c"
      },
      "source": [
        "from pyspark.ml.regression import GeneralizedLinearRegression\n",
        "\n",
        "# Load training data\n",
        "dataset = spark.read.format(\"libsvm\")\\\n",
        "    .load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_linear_regression_data.txt\")\n",
        "\n",
        "glr = GeneralizedLinearRegression(family=\"gaussian\", link=\"identity\", maxIter=10, regParam=0.3)\n",
        "\n",
        "# Fit the model\n",
        "model = glr.fit(dataset)\n",
        "\n",
        "# Print the coefficients and intercept for generalized linear regression model\n",
        "print(\"Coefficients: \" + str(model.coefficients))\n",
        "print(\"Intercept: \" + str(model.intercept))\n",
        "\n",
        "# Summarize the model over the training set and print out some metrics\n",
        "summary = model.summary\n",
        "print(\"Coefficient Standard Errors: \" + str(summary.coefficientStandardErrors))\n",
        "print(\"T Values: \" + str(summary.tValues))\n",
        "print(\"P Values: \" + str(summary.pValues))\n",
        "print(\"Dispersion: \" + str(summary.dispersion))\n",
        "print(\"Null Deviance: \" + str(summary.nullDeviance))\n",
        "print(\"Residual Degree Of Freedom Null: \" + str(summary.residualDegreeOfFreedomNull))\n",
        "print(\"Deviance: \" + str(summary.deviance))\n",
        "print(\"Residual Degree Of Freedom: \" + str(summary.residualDegreeOfFreedom))\n",
        "print(\"AIC: \" + str(summary.aic))\n",
        "print(\"Deviance Residuals: \")\n",
        "summary.residuals().show()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Coefficients: [0.010541828081257216,0.8003253100560949,-0.7845165541420371,2.3679887171421914,0.5010002089857577,1.1222351159753026,-0.2926824398623296,-0.49837174323213035,-0.6035797180675657,0.6725550067187461]\n",
            "Intercept: 0.14592176145232041\n",
            "Coefficient Standard Errors: [0.7950428434287478, 0.8049713176546897, 0.7975916824772489, 0.8312649247659919, 0.7945436200517938, 0.8118992572197593, 0.7919506385542777, 0.7973378214726764, 0.8300714999626418, 0.7771333489686802, 0.463930109648428]\n",
            "T Values: [0.013259446542269243, 0.9942283563442594, -0.9836067393599172, 2.848657084633759, 0.6305509179635714, 1.382234441029355, -0.3695715687490668, -0.6250446546128238, -0.7271418403049983, 0.8654306337661122, 0.31453393176593286]\n",
            "P Values: [0.989426199114056, 0.32060241580811044, 0.3257943227369877, 0.004575078538306521, 0.5286281628105467, 0.16752945248679119, 0.7118614002322872, 0.5322327097421431, 0.467486325282384, 0.3872259825794293, 0.753249430501097]\n",
            "Dispersion: 105.60988356821714\n",
            "Null Deviance: 53229.3654338832\n",
            "Residual Degree Of Freedom Null: 500\n",
            "Deviance: 51748.8429484264\n",
            "Residual Degree Of Freedom: 490\n",
            "AIC: 3769.1895871765314\n",
            "Deviance Residuals: \n",
            "+-------------------+\n",
            "|  devianceResiduals|\n",
            "+-------------------+\n",
            "|-10.974359174246889|\n",
            "| 0.8872320138420559|\n",
            "| -4.596541837478908|\n",
            "|-20.411667435019638|\n",
            "|-10.270419345342642|\n",
            "|-6.0156058956799905|\n",
            "|-10.663939415849267|\n",
            "| 2.1153960525024713|\n",
            "| 3.9807132379137675|\n",
            "|-17.225218272069533|\n",
            "| -4.611647633532147|\n",
            "| 6.4176669407698546|\n",
            "| 11.407137945300537|\n",
            "| -20.70176540467664|\n",
            "| -2.683748540510967|\n",
            "|-16.755494794232536|\n",
            "|  8.154668342638725|\n",
            "|-1.4355057987358848|\n",
            "|-0.6435058688185704|\n",
            "|  -1.13802589316832|\n",
            "+-------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTlZ9l8bAhlS",
        "colab_type": "text"
      },
      "source": [
        "## Decision tree regression\n",
        "\n",
        "Decision trees are a popular family of classification and regression methods. More information about the spark.ml implementation can be found further in the section on decision trees.\n",
        "\n",
        "The following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set. We use a feature transformer to index categorical features, adding metadata to the DataFrame which the Decision Tree algorithm can recognize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFUQypN_AhlW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "1809f3e7-d676-4d66-c25e-1aa1fa9932e7"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import DecisionTreeRegressor\n",
        "from pyspark.ml.feature import VectorIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Load the data stored in LIBSVM format as a DataFrame.\n",
        "data = spark.read.format(\"libsvm\").load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "# Automatically identify categorical features, and index them.\n",
        "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a DecisionTree model.\n",
        "dt = DecisionTreeRegressor(featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexer and tree in a Pipeline\n",
        "pipeline = Pipeline(stages=[featureIndexer, dt])\n",
        "\n",
        "# Train model.  This also runs the indexer.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "\n",
        "treeModel = model.stages[1]\n",
        "# summary only\n",
        "print(treeModel)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.0|  0.0|(692,[123,124,125...|\n",
            "|       0.0|  0.0|(692,[123,124,125...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "|       0.0|  0.0|(692,[126,127,128...|\n",
            "|       0.0|  0.0|(692,[127,128,129...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE) on test data = 0.208514\n",
            "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_69b391725d82) of depth 2 with 5 nodes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYl6Hy5oAhle",
        "colab_type": "text"
      },
      "source": [
        "## Random forest regression\n",
        "\n",
        "Random forests are a popular family of classification and regression methods. More information about the spark.ml implementation can be found further in the section on random forests.\n",
        "\n",
        "The following examples load a dataset in LibSVM format, split it into training and test sets, train on the first dataset, and then evaluate on the held-out test set. We use a feature transformer to index categorical features, adding metadata to the DataFrame which the tree-based algorithms can recognize."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68vqtVT-Ahlg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "d91c9303-f63a-4c08-fbc7-f84161f25470"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import RandomForestRegressor\n",
        "from pyspark.ml.feature import VectorIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Load and parse the data file, converting it to a DataFrame.\n",
        "data = spark.read.format(\"libsvm\").load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "# Automatically identify categorical features, and index them.\n",
        "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a RandomForest model.\n",
        "rf = RandomForestRegressor(featuresCol=\"indexedFeatures\")\n",
        "\n",
        "# Chain indexer and forest in a Pipeline\n",
        "pipeline = Pipeline(stages=[featureIndexer, rf])\n",
        "\n",
        "# Train model.  This also runs the indexer.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "\n",
        "rfModel = model.stages[1]\n",
        "print(rfModel)  # summary only"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.0|  0.0|(692,[123,124,125...|\n",
            "|       0.0|  0.0|(692,[123,124,125...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "|       0.0|  0.0|(692,[126,127,128...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE) on test data = 0.113448\n",
            "RandomForestRegressionModel (uid=RandomForestRegressor_ead14c9e9f6b) with 20 trees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_XHIkTpAhlq",
        "colab_type": "text"
      },
      "source": [
        "## Gradient-boosted tree regression\n",
        "\n",
        "Gradient-boosted trees (GBTs) are a popular regression method using ensembles of decision trees. More information about the spark.ml implementation can be found further in the section on GBTs.\n",
        "\n",
        "Note: For this example dataset, GBTRegressor actually only needs 1 iteration, but that will not be true in general."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "un3MmalbAhls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "outputId": "28ca2209-6c39-4823-c9ae-e183730bb4d9"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.regression import GBTRegressor\n",
        "from pyspark.ml.feature import VectorIndexer\n",
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "\n",
        "# Load and parse the data file, converting it to a DataFrame.\n",
        "data = spark.read.format(\"libsvm\").load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_libsvm_data.txt\")\n",
        "\n",
        "# Automatically identify categorical features, and index them.\n",
        "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
        "featureIndexer =\\\n",
        "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(data)\n",
        "\n",
        "# Split the data into training and test sets (30% held out for testing)\n",
        "(trainingData, testData) = data.randomSplit([0.7, 0.3])\n",
        "\n",
        "# Train a GBT model.\n",
        "gbt = GBTRegressor(featuresCol=\"indexedFeatures\", maxIter=10)\n",
        "\n",
        "# Chain indexer and GBT in a Pipeline\n",
        "pipeline = Pipeline(stages=[featureIndexer, gbt])\n",
        "\n",
        "# Train model.  This also runs the indexer.\n",
        "model = pipeline.fit(trainingData)\n",
        "\n",
        "# Make predictions.\n",
        "predictions = model.transform(testData)\n",
        "\n",
        "# Select example rows to display.\n",
        "predictions.select(\"prediction\", \"label\", \"features\").show(5)\n",
        "\n",
        "# Select (prediction, true label) and compute test error\n",
        "evaluator = RegressionEvaluator(\n",
        "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n",
        "\n",
        "gbtModel = model.stages[1]\n",
        "print(gbtModel)  # summary only"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+----------+-----+--------------------+\n",
            "|prediction|label|            features|\n",
            "+----------+-----+--------------------+\n",
            "|       0.0|  0.0|(692,[122,123,124...|\n",
            "|       0.0|  0.0|(692,[122,123,148...|\n",
            "|       0.0|  0.0|(692,[123,124,125...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "|       0.0|  0.0|(692,[124,125,126...|\n",
            "+----------+-----+--------------------+\n",
            "only showing top 5 rows\n",
            "\n",
            "Root Mean Squared Error (RMSE) on test data = 0.179605\n",
            "GBTRegressionModel (uid=GBTRegressor_63de1cb02b3b) with 10 trees\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKPY75krAhlz",
        "colab_type": "text"
      },
      "source": [
        "## Collaborative filtering\n",
        "\n",
        "Collaborative filtering is commonly used for recommender systems. These techniques aim to fill in the missing entries of a user-item association matrix. spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries. spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14E0LgdOAhl3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3d7274ba-bcbf-4abe-b497-2db56d84839d"
      },
      "source": [
        "from pyspark.ml.evaluation import RegressionEvaluator\n",
        "from pyspark.ml.recommendation import ALS\n",
        "from pyspark.sql import Row\n",
        "\n",
        "lines = spark.read.text(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/als/sample_movielens_ratings.txt\").rdd\n",
        "parts = lines.map(lambda row: row.value.split(\"::\"))\n",
        "ratingsRDD = parts.map(lambda p: Row(userId=int(p[0]), movieId=int(p[1]),\n",
        "                                     rating=float(p[2]), timestamp=int(p[3])))\n",
        "ratings = spark.createDataFrame(ratingsRDD)\n",
        "(training, test) = ratings.randomSplit([0.8, 0.2])\n",
        "\n",
        "# Build the recommendation model using ALS on the training data\n",
        "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\")\n",
        "model = als.fit(training)\n",
        "\n",
        "# Evaluate the model by computing the RMSE on the test data\n",
        "predictions = model.transform(test)\n",
        "evaluator = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
        "                                predictionCol=\"prediction\")\n",
        "rmse = evaluator.evaluate(predictions)\n",
        "print(\"Root-mean-square error = \" + str(rmse))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Root-mean-square error = 1.9254119267846028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBA2lTn2Ahl-",
        "colab_type": "text"
      },
      "source": [
        "## K-means\n",
        "\n",
        "k-means is one of the most commonly used clustering algorithms that clusters the data points into a predefined number of clusters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnvY5lWnAhl_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "9634a73d-5213-4cef-97e7-75e78e91dce5"
      },
      "source": [
        "from pyspark.ml.clustering import KMeans\n",
        "\n",
        "# Loads data.\n",
        "dataset = spark.read.format(\"libsvm\").load(r\"/content/spark-2.4.5-bin-hadoop2.7/data/mllib/sample_kmeans_data.txt\")\n",
        "\n",
        "# Trains a k-means model.\n",
        "kmeans = KMeans().setK(2).setSeed(1)\n",
        "model = kmeans.fit(dataset)\n",
        "\n",
        "# Evaluate clustering by computing Within Set Sum of Squared Errors.\n",
        "wssse = model.computeCost(dataset)\n",
        "print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
        "\n",
        "# Shows the result.\n",
        "centers = model.clusterCenters()\n",
        "print(\"Cluster Centers: \")\n",
        "for center in centers:\n",
        "    print(center)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Within Set Sum of Squared Errors = 0.11999999999994547\n",
            "Cluster Centers: \n",
            "[0.1 0.1 0.1]\n",
            "[9.1 9.1 9.1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}